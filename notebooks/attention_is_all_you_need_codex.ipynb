{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd8903d",
   "metadata": {},
   "source": [
    "\n",
    "# Attention Is All You Need — Exploratory Notebook\n",
    "\n",
    "This notebook walks through the core ideas of Vaswani et al.'s *Attention Is All You Need* paper. Rather than jumping straight to a finished library, we incrementally build reusable pieces — much like the fast.ai approach of exploratory programming — and compose them into a functioning miniature Transformer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4dc7f",
   "metadata": {},
   "source": [
    "\n",
    "## Notebook Roadmap\n",
    "\n",
    "We will iterate through the architecture in the same order the paper introduces it:\n",
    "\n",
    "1. Build toy tokenisation utilities for a miniature sequence-to-sequence task.\n",
    "2. Implement embedding lookup and sinusoidal positional encodings.\n",
    "3. Explore scaled dot-product attention and attention masking.\n",
    "4. Compose multi-head attention from the primitive operation.\n",
    "5. Add the position-wise feed-forward network and residual + layer norm glue.\n",
    "6. Stack encoder and decoder blocks to obtain a full Transformer.\n",
    "7. Run a bite-sized experiment to learn a synthetic translation task, demonstrating the model's end-to-end behaviour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae867f3",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Toy tokenisation utilities\n",
    "\n",
    "The original paper trains on large corpora, but we only need a tiny dataset to exercise the components. We define a mini parallel corpus and helper functions that map between text and token IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d62a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toy_pairs = [\n",
    "    (\"i like deep learning\", \"ich mag tiefes lernen\"),\n",
    "    (\"this is a tiny dataset\", \"dies ist ein winziger datensatz\"),\n",
    "    (\"attention helps models focus\", \"aufmerksamkeit hilft modellen fokus\"),\n",
    "    (\"transformers replace recurrence\", \"transformer ersetzen rekurrenz\"),\n",
    "    (\"we build modules stepwise\", \"wir bauen module schrittweise\"),\n",
    "    (\"layers communicate with attention\", \"schichten kommunizieren mit aufmerksamkeit\"),\n",
    "]\n",
    "\n",
    "SPECIAL_TOKENS = [\"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "src_vocab = {token: idx for idx, token in enumerate(SPECIAL_TOKENS)}\n",
    "tgt_vocab = src_vocab.copy()\n",
    "\n",
    "for src, tgt in toy_pairs:\n",
    "    for token in src.split():\n",
    "        if token not in src_vocab:\n",
    "            src_vocab[token] = len(src_vocab)\n",
    "    for token in tgt.split():\n",
    "        if token not in tgt_vocab:\n",
    "            tgt_vocab[token] = len(tgt_vocab)\n",
    "\n",
    "inv_src_vocab = {idx: token for token, idx in src_vocab.items()}\n",
    "inv_tgt_vocab = {idx: token for token, idx in tgt_vocab.items()}\n",
    "\n",
    "src_vocab_size, tgt_vocab_size = len(src_vocab), len(tgt_vocab)\n",
    "print(f\"Source vocab: {src_vocab_size} tokens | Target vocab: {tgt_vocab_size} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c002ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode(sentence: str, vocab: dict) -> torch.Tensor:\n",
    "    return torch.tensor([vocab[token] for token in sentence.split()], dtype=torch.long)\n",
    "\n",
    "\n",
    "def decode(ids: torch.Tensor, inv_vocab: dict) -> str:\n",
    "    tokens = [inv_vocab[i] for i in ids.tolist()]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "encoded_example = encode(toy_pairs[0][0], src_vocab)\n",
    "print(encoded_example)\n",
    "print(decode(encoded_example, inv_src_vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77442d97",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Embeddings with sinusoidal positional encoding\n",
    "\n",
    "The paper replaces recurrence with positional signals. We'll implement the sinusoidal encoding that can be added to learned token embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9233025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.pe[:, : x.size(1)]\n",
    "\n",
    "\n",
    "embedding_dim = 32\n",
    "src_embed = nn.Embedding(src_vocab_size, embedding_dim)\n",
    "tgt_embed = nn.Embedding(tgt_vocab_size, embedding_dim)\n",
    "positional_encoding = PositionalEncoding(embedding_dim)\n",
    "\n",
    "sample = src_embed(encoded_example.unsqueeze(0))\n",
    "print(\"Embedding shape:\", sample.shape)\n",
    "print(\"With positional encoding:\", positional_encoding(sample).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f70450",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Scaled dot-product attention\n",
    "\n",
    "Scaled dot-product attention maps queries, keys, and values to contextualised representations. We'll implement it directly from the paper and probe it with a small example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    d_k = query.size(-1)\n",
    "    scores = query @ key.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = attn_weights @ value\n",
    "    return output, attn_weights\n",
    "\n",
    "\n",
    "q = torch.randn(1, 4, embedding_dim)\n",
    "k = torch.randn(1, 4, embedding_dim)\n",
    "v = torch.randn(1, 4, embedding_dim)\n",
    "\n",
    "context, weights = scaled_dot_product_attention(q, k, v)\n",
    "print(\"Context shape:\", context.shape)\n",
    "print(\"Attention weights row sums:\", weights.sum(-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702ddd11",
   "metadata": {},
   "source": [
    "\n",
    "Masks prevent information leakage in the decoder. We can reuse the same primitive with different masks for padding and future tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655d62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def subsequent_mask(size: int) -> torch.Tensor:\n",
    "    mask = torch.triu(torch.ones(size, size, dtype=torch.bool), diagonal=1)\n",
    "    return (~mask).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "mask = subsequent_mask(5)\n",
    "print(mask[0, 0].int())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19073d0",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Multi-head attention as a composition\n",
    "\n",
    "Now that we trust the scaled attention primitive, we wrap it into the multi-head structure with learned projections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        batch_size = query.size(0)\n",
    "        q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        attn_output, _ = scaled_dot_product_attention(q, k, v, mask)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "\n",
    "mha = MultiHeadAttention(d_model=embedding_dim, num_heads=4)\n",
    "x = torch.randn(2, 6, embedding_dim)\n",
    "print(mha(x, x, x).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7939ea9",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Position-wise feed-forward network and residual glue\n",
    "\n",
    "Each encoder/decoder layer applies a two-layer MLP with a ReLU in between, plus residual connections and layer normalisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0843075",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int = 128):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, sublayer) -> torch.Tensor:\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "tensor = torch.randn(3, 4, embedding_dim)\n",
    "ff = PositionwiseFeedForward(embedding_dim)\n",
    "residual = ResidualConnection(embedding_dim)\n",
    "print(residual(tensor, ff).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bfb514",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Encoder and decoder blocks\n",
    "\n",
    "We can now assemble encoder and decoder layers, mirroring Figures 1 and 2 from the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86590fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
    "        self.sublayers = nn.ModuleList(\n",
    "            [ResidualConnection(d_model, dropout) for _ in range(2)]\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, src_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.sublayers[0](x, lambda x_: self.self_attn(x_, x_, x_, src_mask))\n",
    "        x = self.sublayers[1](x, self.feed_forward)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.src_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
    "        self.sublayers = nn.ModuleList(\n",
    "            [ResidualConnection(d_model, dropout) for _ in range(3)]\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: Optional[torch.Tensor] = None,\n",
    "        memory_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.sublayers[0](x, lambda x_: self.self_attn(x_, x_, x_, tgt_mask))\n",
    "        x = self.sublayers[1](x, lambda x_: self.src_attn(x_, memory, memory, memory_mask))\n",
    "        x = self.sublayers[2](x, self.feed_forward)\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_layer = EncoderLayer(embedding_dim, num_heads=4, d_ff=128)\n",
    "decoder_layer = DecoderLayer(embedding_dim, num_heads=4, d_ff=128)\n",
    "\n",
    "memory = encoder_layer(torch.randn(2, 5, embedding_dim))\n",
    "output = decoder_layer(torch.randn(2, 6, embedding_dim), memory)\n",
    "print(memory.shape, output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df9de83",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Stacking layers into a mini Transformer\n",
    "\n",
    "With layers in place, we construct encoder and decoder stacks plus the final linear generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer: EncoderLayer, N: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                layer if i == 0 else EncoderLayer(\n",
    "                    layer.self_attn.d_model,\n",
    "                    layer.self_attn.num_heads,\n",
    "                    layer.feed_forward.linear1.out_features,\n",
    "                )\n",
    "                for i in range(N)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(layer.self_attn.d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer: DecoderLayer, N: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                layer if i == 0 else DecoderLayer(\n",
    "                    layer.self_attn.d_model,\n",
    "                    layer.self_attn.num_heads,\n",
    "                    layer.feed_forward.linear1.out_features,\n",
    "                )\n",
    "                for i in range(N)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(layer.self_attn.d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: Optional[torch.Tensor] = None,\n",
    "        memory_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask, memory_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab: int,\n",
    "        tgt_vocab: int,\n",
    "        d_model: int = 64,\n",
    "        num_heads: int = 4,\n",
    "        d_ff: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.src_embed = nn.Embedding(src_vocab, d_model)\n",
    "        self.tgt_embed = nn.Embedding(tgt_vocab, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layer = EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "        decoder_layer = DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "        self.encoder = Encoder(encoder_layer, num_layers)\n",
    "        self.decoder = Decoder(decoder_layer, num_layers)\n",
    "        self.generator = nn.Linear(d_model, tgt_vocab)\n",
    "\n",
    "    def encode(self, src: torch.Tensor, src_mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        return self.encoder(self.positional_encoding(self.src_embed(src)), src_mask)\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        tgt_mask: Optional[torch.Tensor],\n",
    "        memory_mask: Optional[torch.Tensor],\n",
    "    ):\n",
    "        return self.decoder(\n",
    "            self.positional_encoding(self.tgt_embed(tgt)),\n",
    "            memory,\n",
    "            tgt_mask,\n",
    "            memory_mask,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        tgt: torch.Tensor,\n",
    "        src_mask: Optional[torch.Tensor],\n",
    "        tgt_mask: Optional[torch.Tensor],\n",
    "        memory_mask: Optional[torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        memory = self.encode(src, src_mask)\n",
    "        output = self.decode(tgt, memory, tgt_mask, memory_mask)\n",
    "        return self.generator(output)\n",
    "\n",
    "\n",
    "model = Transformer(src_vocab_size, tgt_vocab_size, d_model=embedding_dim)\n",
    "src_batch = torch.stack([encode(pair[0], src_vocab) for pair in toy_pairs])\n",
    "tgt_batch = torch.stack([encode(pair[1], tgt_vocab) for pair in toy_pairs])\n",
    "print(model(src_batch, tgt_batch, None, None, None).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6fd19",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Training on a synthetic translation task\n",
    "\n",
    "To verify the notebook components, we run a tiny training loop. The goal is simply to overfit the six toy sentence pairs so that the decoder learns to predict the next token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_batch(pairs):\n",
    "    src_seqs, tgt_seqs = [], []\n",
    "    for src, tgt in pairs:\n",
    "        src_ids = encode(src, src_vocab)\n",
    "        tgt_ids = torch.cat(\n",
    "            [\n",
    "                torch.tensor([tgt_vocab[\"<bos>\"]]),\n",
    "                encode(tgt, tgt_vocab),\n",
    "                torch.tensor([tgt_vocab[\"<eos>\"]]),\n",
    "            ]\n",
    "        )\n",
    "        src_seqs.append(src_ids)\n",
    "        tgt_seqs.append(tgt_ids)\n",
    "    src_pad = nn.utils.rnn.pad_sequence(\n",
    "        src_seqs, batch_first=True, padding_value=src_vocab[\"<pad>\"]\n",
    "    )\n",
    "    tgt_pad = nn.utils.rnn.pad_sequence(\n",
    "        tgt_seqs, batch_first=True, padding_value=tgt_vocab[\"<pad>\"]\n",
    "    )\n",
    "    return src_pad, tgt_pad\n",
    "\n",
    "\n",
    "src_batch, tgt_batch = make_batch(toy_pairs)\n",
    "print(\"Source batch shape:\", src_batch.shape)\n",
    "print(\"Target batch shape:\", tgt_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_masks(src: torch.Tensor, tgt: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    src_mask = (src != src_vocab[\"<pad>\"]).unsqueeze(1).unsqueeze(2)\n",
    "    tgt_mask = (tgt != tgt_vocab[\"<pad>\"]).unsqueeze(1).unsqueeze(2)\n",
    "    size = tgt.size(1)\n",
    "    tgt_mask = tgt_mask & subsequent_mask(size)\n",
    "    memory_mask = src_mask\n",
    "    return src_mask, tgt_mask, memory_mask\n",
    "\n",
    "\n",
    "src_mask, tgt_mask, memory_mask = create_masks(src_batch, tgt_batch)\n",
    "print(src_mask.shape, tgt_mask.shape, memory_mask.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a04b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size,\n",
    "    tgt_vocab_size,\n",
    "    d_model=embedding_dim,\n",
    "    num_heads=4,\n",
    "    d_ff=128,\n",
    "    num_layers=2,\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab[\"<pad>\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(\n",
    "        src_batch,\n",
    "        tgt_batch[:, :-1],\n",
    "        src_mask,\n",
    "        tgt_mask[:, :, :, :-1],\n",
    "        memory_mask,\n",
    "    )\n",
    "    loss = criterion(\n",
    "        logits.reshape(-1, tgt_vocab_size),\n",
    "        tgt_batch[:, 1:].reshape(-1),\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}: loss={loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2adafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def greedy_decode(model: Transformer, src_sentence: str, max_len: int = 20) -> str:\n",
    "    model.eval()\n",
    "    src = encode(src_sentence, src_vocab).unsqueeze(0)\n",
    "    src_mask = (src != src_vocab[\"<pad>\"]).unsqueeze(1).unsqueeze(2)\n",
    "    memory = model.encode(src, src_mask)\n",
    "\n",
    "    ys = torch.tensor([[tgt_vocab[\"<bos>\"]]])\n",
    "    for _ in range(max_len):\n",
    "        tgt_mask = subsequent_mask(ys.size(1)).to(ys.device)\n",
    "        out = model.decode(ys, memory, tgt_mask, src_mask)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "        ys = torch.cat([ys, torch.tensor([[next_word]])], dim=1)\n",
    "        if next_word == tgt_vocab[\"<eos>\"]:\n",
    "            break\n",
    "    return decode(ys[0, 1:-1], inv_tgt_vocab)\n",
    "\n",
    "\n",
    "for src, tgt in toy_pairs:\n",
    "    prediction = greedy_decode(model, src)\n",
    "    print(f\"SRC: {src}\n",
    "PRED: {prediction}\n",
    "TGT: {tgt}\n",
    "\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
